{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pFHxIL9yv07"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bT8r4RGyv1A"
   },
   "source": [
    "# Demo: CNN with Keras\n",
    "INSTRUCTIONS:\n",
    "- Run the cells\n",
    "- Observe and understand the results\n",
    "- Answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dfgYO-oyv1D"
   },
   "source": [
    "## Faces classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH3F0aLlyv1H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmF9-C-syv1Q"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0DhL5fvqyv1T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import libraries\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFuJDWijyv1Y"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpYq76vyyv1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[0.30991736, 0.3677686 , 0.41735536, ..., 0.15289256, 0.16115703,\n",
       "         0.1570248 ],\n",
       "        [0.45454547, 0.47107437, 0.5123967 , ..., 0.15289256, 0.15289256,\n",
       "         0.15289256],\n",
       "        [0.3181818 , 0.40082645, 0.49173555, ..., 0.14049587, 0.14876033,\n",
       "         0.15289256],\n",
       "        ...,\n",
       "        [0.5       , 0.53305787, 0.607438  , ..., 0.17768595, 0.14876033,\n",
       "         0.19008264],\n",
       "        [0.21487603, 0.21900827, 0.21900827, ..., 0.57438016, 0.59090906,\n",
       "         0.60330576],\n",
       "        [0.5165289 , 0.46280992, 0.28099173, ..., 0.35950413, 0.3553719 ,\n",
       "         0.38429752]], dtype=float32),\n",
       " 'images': array([[[0.30991736, 0.3677686 , 0.41735536, ..., 0.37190083,\n",
       "          0.3305785 , 0.30578512],\n",
       "         [0.3429752 , 0.40495867, 0.43801653, ..., 0.37190083,\n",
       "          0.338843  , 0.3140496 ],\n",
       "         [0.3429752 , 0.41735536, 0.45041323, ..., 0.38016528,\n",
       "          0.338843  , 0.29752067],\n",
       "         ...,\n",
       "         [0.21487603, 0.20661157, 0.2231405 , ..., 0.15289256,\n",
       "          0.16528925, 0.17355372],\n",
       "         [0.20247933, 0.2107438 , 0.2107438 , ..., 0.14876033,\n",
       "          0.16115703, 0.16528925],\n",
       "         [0.20247933, 0.20661157, 0.20247933, ..., 0.15289256,\n",
       "          0.16115703, 0.1570248 ]],\n",
       " \n",
       "        [[0.45454547, 0.47107437, 0.5123967 , ..., 0.19008264,\n",
       "          0.18595041, 0.18595041],\n",
       "         [0.446281  , 0.48347107, 0.5206612 , ..., 0.21487603,\n",
       "          0.2107438 , 0.2107438 ],\n",
       "         [0.49586776, 0.5165289 , 0.53305787, ..., 0.20247933,\n",
       "          0.20661157, 0.20661157],\n",
       "         ...,\n",
       "         [0.77272725, 0.78099173, 0.7933884 , ..., 0.1446281 ,\n",
       "          0.1446281 , 0.1446281 ],\n",
       "         [0.77272725, 0.7768595 , 0.7892562 , ..., 0.13636364,\n",
       "          0.13636364, 0.13636364],\n",
       "         [0.7644628 , 0.7892562 , 0.78099173, ..., 0.15289256,\n",
       "          0.15289256, 0.15289256]],\n",
       " \n",
       "        [[0.3181818 , 0.40082645, 0.49173555, ..., 0.40082645,\n",
       "          0.3553719 , 0.30991736],\n",
       "         [0.30991736, 0.3966942 , 0.47933885, ..., 0.40495867,\n",
       "          0.37603307, 0.30165288],\n",
       "         [0.26859504, 0.34710744, 0.45454547, ..., 0.3966942 ,\n",
       "          0.37190083, 0.30991736],\n",
       "         ...,\n",
       "         [0.1322314 , 0.09917355, 0.08264463, ..., 0.13636364,\n",
       "          0.14876033, 0.15289256],\n",
       "         [0.11570248, 0.09504132, 0.0785124 , ..., 0.1446281 ,\n",
       "          0.1446281 , 0.1570248 ],\n",
       "         [0.11157025, 0.09090909, 0.0785124 , ..., 0.14049587,\n",
       "          0.14876033, 0.15289256]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0.5       , 0.53305787, 0.607438  , ..., 0.28512397,\n",
       "          0.23966943, 0.21487603],\n",
       "         [0.49173555, 0.5413223 , 0.60330576, ..., 0.29752067,\n",
       "          0.20247933, 0.20661157],\n",
       "         [0.46694216, 0.55785125, 0.6198347 , ..., 0.29752067,\n",
       "          0.17768595, 0.18595041],\n",
       "         ...,\n",
       "         [0.03305785, 0.46280992, 0.5289256 , ..., 0.17355372,\n",
       "          0.17355372, 0.1694215 ],\n",
       "         [0.1570248 , 0.5247934 , 0.53305787, ..., 0.16528925,\n",
       "          0.1570248 , 0.18595041],\n",
       "         [0.45454547, 0.5206612 , 0.53305787, ..., 0.17768595,\n",
       "          0.14876033, 0.19008264]],\n",
       " \n",
       "        [[0.21487603, 0.21900827, 0.21900827, ..., 0.71487606,\n",
       "          0.71487606, 0.6942149 ],\n",
       "         [0.20247933, 0.20661157, 0.20661157, ..., 0.7107438 ,\n",
       "          0.7066116 , 0.6942149 ],\n",
       "         [0.2107438 , 0.20661157, 0.20661157, ..., 0.6859504 ,\n",
       "          0.69008267, 0.6942149 ],\n",
       "         ...,\n",
       "         [0.2644628 , 0.25619835, 0.2603306 , ..., 0.5413223 ,\n",
       "          0.57438016, 0.59090906],\n",
       "         [0.26859504, 0.2644628 , 0.26859504, ..., 0.56198347,\n",
       "          0.58264464, 0.59504133],\n",
       "         [0.27272728, 0.26859504, 0.27272728, ..., 0.57438016,\n",
       "          0.59090906, 0.60330576]],\n",
       " \n",
       "        [[0.5165289 , 0.46280992, 0.28099173, ..., 0.5785124 ,\n",
       "          0.5413223 , 0.60330576],\n",
       "         [0.5165289 , 0.45041323, 0.29338843, ..., 0.58264464,\n",
       "          0.553719  , 0.5785124 ],\n",
       "         [0.5165289 , 0.44214877, 0.29338843, ..., 0.59917355,\n",
       "          0.5785124 , 0.54545456],\n",
       "         ...,\n",
       "         [0.39256197, 0.41322315, 0.38842976, ..., 0.33471075,\n",
       "          0.37190083, 0.3966942 ],\n",
       "         [0.39256197, 0.38429752, 0.40495867, ..., 0.3305785 ,\n",
       "          0.35950413, 0.37603307],\n",
       "         [0.3677686 , 0.40495867, 0.3966942 , ..., 0.35950413,\n",
       "          0.3553719 , 0.38429752]]], dtype=float32),\n",
       " 'target': array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "        11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15,\n",
       "        15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
       "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18,\n",
       "        18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
       "        22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,\n",
       "        23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25,\n",
       "        25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27,\n",
       "        27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "        28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30,\n",
       "        30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32,\n",
       "        32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33,\n",
       "        34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35,\n",
       "        35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37,\n",
       "        37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39,\n",
       "        39, 39, 39, 39, 39, 39, 39, 39, 39]),\n",
       " 'DESCR': '.. _olivetti_faces_dataset:\\n\\nThe Olivetti faces dataset\\n--------------------------\\n\\n`This dataset contains a set of face images`_ taken between April 1992 and \\nApril 1994 at AT&T Laboratories Cambridge. The\\n:func:`sklearn.datasets.fetch_olivetti_faces` function is the data\\nfetching / caching function that downloads the data\\narchive from AT&T.\\n\\n.. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\\n\\nAs described on the original website:\\n\\n    There are ten different images of each of 40 distinct subjects. For some\\n    subjects, the images were taken at different times, varying the lighting,\\n    facial expressions (open / closed eyes, smiling / not smiling) and facial\\n    details (glasses / no glasses). All the images were taken against a dark\\n    homogeneous background with the subjects in an upright, frontal position \\n    (with tolerance for some side movement).\\n\\n**Data Set Characteristics:**\\n\\n    =================   =====================\\n    Classes                                40\\n    Samples total                         400\\n    Dimensionality                       4096\\n    Features            real, between 0 and 1\\n    =================   =====================\\n\\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit \\nintegers; the loader will convert these to floating point values on the \\ninterval [0, 1], which are easier to work with for many algorithms.\\n\\nThe \"target\" for this database is an integer from 0 to 39 indicating the\\nidentity of the person pictured; however, with only 10 examples per class, this\\nrelatively small dataset is more interesting from an unsupervised or\\nsemi-supervised perspective.\\n\\nThe original dataset consisted of 92 x 112, while the version available here\\nconsists of 64x64 images.\\n\\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\\n'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The faces dataset\n",
    "faces = fetch_olivetti_faces()\n",
    "faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnadpkG2yv1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _olivetti_faces_dataset:\n",
      "\n",
      "The Olivetti faces dataset\n",
      "--------------------------\n",
      "\n",
      "`This dataset contains a set of face images`_ taken between April 1992 and \n",
      "April 1994 at AT&T Laboratories Cambridge. The\n",
      ":func:`sklearn.datasets.fetch_olivetti_faces` function is the data\n",
      "fetching / caching function that downloads the data\n",
      "archive from AT&T.\n",
      "\n",
      ".. _This dataset contains a set of face images: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html\n",
      "\n",
      "As described on the original website:\n",
      "\n",
      "    There are ten different images of each of 40 distinct subjects. For some\n",
      "    subjects, the images were taken at different times, varying the lighting,\n",
      "    facial expressions (open / closed eyes, smiling / not smiling) and facial\n",
      "    details (glasses / no glasses). All the images were taken against a dark\n",
      "    homogeneous background with the subjects in an upright, frontal position \n",
      "    (with tolerance for some side movement).\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   =====================\n",
      "    Classes                                40\n",
      "    Samples total                         400\n",
      "    Dimensionality                       4096\n",
      "    Features            real, between 0 and 1\n",
      "    =================   =====================\n",
      "\n",
      "The image is quantized to 256 grey levels and stored as unsigned 8-bit \n",
      "integers; the loader will convert these to floating point values on the \n",
      "interval [0, 1], which are easier to work with for many algorithms.\n",
      "\n",
      "The \"target\" for this database is an integer from 0 to 39 indicating the\n",
      "identity of the person pictured; however, with only 10 examples per class, this\n",
      "relatively small dataset is more interesting from an unsupervised or\n",
      "semi-supervised perspective.\n",
      "\n",
      "The original dataset consisted of 92 x 112, while the version available here\n",
      "consists of 64x64 images.\n",
      "\n",
      "When using these images, please give credit to AT&T Laboratories Cambridge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(faces.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wO-oI0WUyv1k"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id = np.random.randint(len(faces.target))\n",
    "plt.figure(figsize = (3, 3))\n",
    "plt.imshow(faces.images[id], cmap = 'gray')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3rwC2rdyv1n"
   },
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "# input\n",
    "X = faces.images\n",
    "img_rows, img_cols = X[0].shape\n",
    "X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# output\n",
    "target = faces.target.astype(np.uint8)\n",
    "\n",
    "# Convert the target to categorical\n",
    "y = to_categorical(\n",
    "    target,\n",
    "    num_classes = len(set(target)),\n",
    "    dtype = 'uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hEIZT0Pyv1q"
   },
   "outputs": [],
   "source": [
    "filter = []\n",
    "# run in blocks of 10\n",
    "for i in range(len(target) // 10):\n",
    "    s = set()\n",
    "    while len(s) < 2:\n",
    "        s = set(np.random.randint(0, 10, 2, dtype = np.int8))\n",
    "    a = [x in s for x in range(10)]\n",
    "    filter.append(a)\n",
    "test = np.array(filter).flatten()\n",
    "train = np.array([not t for t in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAwmwl9fyv1s"
   },
   "outputs": [],
   "source": [
    "X_train = X[train].copy()\n",
    "X_test  = X[test].copy()\n",
    "y_train = y[train].copy()\n",
    "y_test  = y[test].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P9oGZCxyv1t"
   },
   "outputs": [],
   "source": [
    "# Set up the model architecture\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMv2DgJtyv1v"
   },
   "outputs": [],
   "source": [
    "# Add a convolutional layer\n",
    "model.add(Conv2D(60,\n",
    "                 kernel_size = 4,\n",
    "                 activation = 'relu',\n",
    "                 data_format = 'channels_last',\n",
    "                 padding = 'valid',\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(30,\n",
    "                 kernel_size = 2,\n",
    "                 activation = 'relu'))\n",
    "\n",
    "# Flatten the output of the convolutional layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add an output layer for the 3 categories\n",
    "model.add(Dense(len(set(target)),\n",
    "                activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QAK8acr9yv1x",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DkcUhjmyv11"
   },
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(optimizer = 'adam', \n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OCxnEJOyv12"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model on a training set\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs = 9,\n",
    "    batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ENovdR5_yv15"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vuw-Lnpuyv17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size = 10)\n",
    "print('\\nTest loss: %.6f, Test accuracy: %.6f' % tuple(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZkGlsB2yv19"
   },
   "outputs": [],
   "source": [
    "def print_cm(cm):\n",
    "    d_size = max(len('%d' % cm.max()), len('%d' % cm.shape[1]))\n",
    "    if min(cm.shape) > 10: # make sparse\n",
    "        print('Sparse Matrix (*=diagonal)')\n",
    "        fmt_r = 'r%%0%dd' % d_size\n",
    "        fmt_c = ', c%%0%dd%%s= %%%dd' % (d_size, d_size)\n",
    "        for i in range(cm.shape[0]):\n",
    "            s = fmt_r % i\n",
    "            for j in range(cm.shape[1]):\n",
    "                if cm[i, j] > 0:\n",
    "                    s += fmt_c % (j, '*' if i == j else ' ', cm[i, j])\n",
    "            print(s)\n",
    "    else: # make dense\n",
    "        c = '%%%dd ' % d_size\n",
    "        s = '%s| ' % (' ' * d_size)\n",
    "        s += ''.join([c % i for i in range(len(cm[0]))])\n",
    "        print(s)\n",
    "        print('-' * len(s))\n",
    "        for i, r in enumerate(cm):\n",
    "            s = '%2d| ' % i\n",
    "            s += c * len(r)\n",
    "            print(s % tuple(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jz1anYwyv1-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test_target = np.array([x.argmax() for x in y_test])\n",
    "cm = confusion_matrix(y_test_target, predictions)\n",
    "print_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ngmNXvOyv2C"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (18, 6))\n",
    "fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(model.history.history['acc'])\n",
    "ax[0].plot(model.history.history['val_acc'])\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend(['Train', 'Validation'])\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(model.history.history['loss'])\n",
    "ax[1].plot(model.history.history['val_loss'])\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend(['Train', 'Validation'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9V0zAzJyv2E"
   },
   "outputs": [],
   "source": [
    "def implot2(im1, im2, id):\n",
    "    t1 = y_test[id].argmax()\n",
    "    t2 = predictions[id]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (6, 3))\n",
    "    fig.subplots_adjust(left = 0.02, right = 0.98, top = 0.85, wspace = 0.2)\n",
    "    fig.suptitle('Prediction %d' % id, fontsize = 12, fontweight = 'bold')\n",
    "\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "    # original image\n",
    "    ax[0].imshow(im1, cmap = 'gray')\n",
    "    ax[0].set_title('Original id: %d' % t1)\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_yticks([])\n",
    "\n",
    "    # convoluted image\n",
    "    ax[1].imshow(im2, cmap = 'gray')\n",
    "    ax[1].set_title('Test id: %d' % t2)\n",
    "    ax[1].set_xticks([])\n",
    "    ax[1].set_yticks([])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9x2SbfxNyv2G"
   },
   "outputs": [],
   "source": [
    "def pick_test_image(pid):\n",
    "    # pick the prediction\n",
    "    ppid = predictions[pid]\n",
    "    # find the corresponding image\n",
    "    j = -1\n",
    "    for ipid in range(test.shape[0]):\n",
    "        if test[ipid]:\n",
    "            j += 1\n",
    "        if j == pid:\n",
    "            break\n",
    "    return X[ipid].reshape(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWkU7UEGyv2J"
   },
   "outputs": [],
   "source": [
    "def pick_train_image(ppid):\n",
    "    oid = np.array([a.argmax() == ppid for a in y_train]).argmax()\n",
    "    poid = y_train[oid].argmax()\n",
    "    j = -1\n",
    "    for ioid in range(train.shape[0]):\n",
    "        if train[ioid]:\n",
    "            j += 1\n",
    "        if j == oid:\n",
    "            break\n",
    "    return X[ioid].reshape(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEnS6712yv2K"
   },
   "outputs": [],
   "source": [
    "def compare_images(id):\n",
    "    XTest = pick_test_image(id)\n",
    "    XTrain = pick_train_image(predictions[id])\n",
    "    implot2(XTrain, XTest, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZEUyv-UAyv2M"
   },
   "outputs": [],
   "source": [
    "# compare one\n",
    "id = np.random.randint(len(predictions))\n",
    "compare_images(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_o2EPD7syv2N",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compare ten\n",
    "ids = np.random.randint(len(predictions), size = 10)\n",
    "for id in ids:\n",
    "    compare_images(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS5Tc4z9FoYy"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxI2We9OFpfs"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81DoNxN1FqGN"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2019 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gofP3k2Heop1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Demo-10-Keras_CNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
