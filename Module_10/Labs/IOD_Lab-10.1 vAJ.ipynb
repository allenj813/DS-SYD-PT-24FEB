{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gn3SG1yo1KdV"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-SD7a9X1KdY"
   },
   "source": [
    "# Lab 10.1: NN with Keras\n",
    "INSTRUCTIONS:\n",
    "- Read the guides and hints, then create the necessary analysis and code to find an answer and conclusion for the task below.\n",
    "- **NOTE**: This is a Regression problem. Consider the appropriate:\n",
    "    - Activation function\n",
    "    - Loss/Cost Function\n",
    "    - Metrics\n",
    "    - This is a regression problem, as opposed to a classification problem (on the demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENgfRnvL1Kdc"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9H465X-1Kde"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n",
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5FHh910J1Kdm"
   },
   "source": [
    "### Load data\n",
    "Load the Diabetes dataset from **SciKit-Learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWu8SlQF1Kdo"
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xR_yLpR01Kdr"
   },
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp2mf2bB1Kds"
   },
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "X = diabetes.data # Get feature using data attribute\n",
    "n_cols = X.shape[1]\n",
    "\n",
    "# Output\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1Vsh6cm1Kdv"
   },
   "source": [
    "### Split the data (training/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O341llJz1Kdw"
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4dJViJD1Kd0"
   },
   "source": [
    "### Create the model's architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tkgf_BLl1Kd1"
   },
   "outputs": [],
   "source": [
    "# Instantiate architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Add first hidden layer, 30 is arbitrary and hyperparameters\n",
    "model.add(Dense(30, activation = 'relu', input_shape = (n_cols, )))\n",
    "\n",
    "# Add second hidden layer, 15 is arbitrary and hyperparameters\n",
    "model.add(Dense(15, activation = 'relu'))\n",
    "\n",
    "# Add third hidden layer, 5 is arbitrary and hyperparameters\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "\n",
    "# Add output layer, use linear because it is regression\n",
    "model.add(Dense(1, activation = 'linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 15)                465       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 5)                 80        \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 881\n",
      "Trainable params: 881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hQTS42V1Kd4"
   },
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvLsoanF1Kd5"
   },
   "outputs": [],
   "source": [
    "# Optimizer tries to minimize the loss function after every epoch\n",
    "# Depends on if regression, classification, etc\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'mse',\n",
    "    metrics = ['mape']) # mean absolute percentage error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KPQbnEj1Kd7"
   },
   "source": [
    "### Fit the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data\n",
    "# from sklearn.utils import shuffle\n",
    "# X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oiuHjEj1Kd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 27200.0840 - mape: 99.9505 - val_loss: 28837.7988 - val_mape: 99.9088\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 27176.8398 - mape: 99.8747 - val_loss: 28807.7363 - val_mape: 99.8120\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 27142.1191 - mape: 99.7653 - val_loss: 28763.9492 - val_mape: 99.6745\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 27092.2988 - mape: 99.6098 - val_loss: 28701.9238 - val_mape: 99.4817\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 27022.1426 - mape: 99.3883 - val_loss: 28614.4785 - val_mape: 99.2112\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 26924.4570 - mape: 99.0838 - val_loss: 28491.9746 - val_mape: 98.8373\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 26789.7422 - mape: 98.6580 - val_loss: 28324.0215 - val_mape: 98.3293\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 26602.7266 - mape: 98.0967 - val_loss: 28099.1699 - val_mape: 97.6480\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 26352.0371 - mape: 97.3568 - val_loss: 27801.4258 - val_mape: 96.7474\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 26025.8867 - mape: 96.3628 - val_loss: 27411.6699 - val_mape: 95.5660\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 25615.6191 - mape: 95.0303 - val_loss: 26911.8535 - val_mape: 94.0568\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 25086.4355 - mape: 93.4153 - val_loss: 26302.4414 - val_mape: 92.1938\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 24438.7578 - mape: 91.4296 - val_loss: 25563.6602 - val_mape: 89.8955\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 23658.7344 - mape: 88.9830 - val_loss: 24670.2656 - val_mape: 87.0796\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 22747.7090 - mape: 85.9088 - val_loss: 23608.0801 - val_mape: 83.6531\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 21659.3945 - mape: 82.2896 - val_loss: 22400.4551 - val_mape: 79.6414\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 20446.9121 - mape: 78.0716 - val_loss: 21023.5586 - val_mape: 74.9189\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 19067.3320 - mape: 73.2990 - val_loss: 19518.6191 - val_mape: 69.5444\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 17546.3066 - mape: 67.9944 - val_loss: 17924.0254 - val_mape: 64.0134\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 15977.1631 - mape: 62.1311 - val_loss: 16206.3447 - val_mape: 58.1118\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 14338.3809 - mape: 55.9586 - val_loss: 14435.7607 - val_mape: 52.8036\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 12669.8506 - mape: 50.0971 - val_loss: 12676.1133 - val_mape: 47.9839\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 11039.7188 - mape: 45.5984 - val_loss: 11011.4229 - val_mape: 45.7709\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 9498.0928 - mape: 42.0129 - val_loss: 9473.7979 - val_mape: 45.2875\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 8148.2104 - mape: 39.9771 - val_loss: 8098.1924 - val_mape: 46.0905\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 6965.9062 - mape: 39.8526 - val_loss: 6957.5376 - val_mape: 47.5587\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 6012.2949 - mape: 40.3415 - val_loss: 6076.9922 - val_mape: 49.6542\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 5346.2031 - mape: 42.2481 - val_loss: 5403.3062 - val_mape: 51.8210\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4843.7812 - mape: 43.8072 - val_loss: 4958.6338 - val_mape: 53.7308\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4538.9194 - mape: 45.8528 - val_loss: 4651.2246 - val_mape: 55.3172\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4352.4717 - mape: 47.4958 - val_loss: 4447.8076 - val_mape: 56.5275\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4207.4160 - mape: 48.4873 - val_loss: 4337.2104 - val_mape: 56.8905\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4141.3433 - mape: 49.3383 - val_loss: 4230.7554 - val_mape: 57.4691\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4066.5198 - mape: 49.8932 - val_loss: 4165.1484 - val_mape: 57.5557\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4020.6995 - mape: 50.1269 - val_loss: 4104.5903 - val_mape: 57.5614\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3975.8745 - mape: 50.0298 - val_loss: 4059.0530 - val_mape: 57.2848\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3937.3257 - mape: 50.0266 - val_loss: 4008.8118 - val_mape: 57.2348\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3897.2874 - mape: 49.8197 - val_loss: 3966.6819 - val_mape: 56.8460\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3861.1057 - mape: 49.3785 - val_loss: 3932.6355 - val_mape: 56.4043\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3827.5791 - mape: 49.0554 - val_loss: 3895.5664 - val_mape: 56.1211\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3795.9392 - mape: 48.8080 - val_loss: 3861.1177 - val_mape: 55.7939\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3765.3372 - mape: 48.6001 - val_loss: 3823.1626 - val_mape: 55.7268\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3735.9502 - mape: 48.3972 - val_loss: 3797.1707 - val_mape: 55.2820\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3707.2976 - mape: 48.0243 - val_loss: 3770.2512 - val_mape: 54.9502\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3682.3096 - mape: 47.8787 - val_loss: 3734.5220 - val_mape: 54.9059\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3654.0535 - mape: 47.7835 - val_loss: 3708.4727 - val_mape: 54.5826\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3632.7358 - mape: 47.6157 - val_loss: 3682.3176 - val_mape: 54.3682\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3609.0034 - mape: 47.5511 - val_loss: 3652.4070 - val_mape: 54.3647\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3585.3467 - mape: 47.4721 - val_loss: 3627.5476 - val_mape: 54.0188\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3561.9849 - mape: 46.9899 - val_loss: 3608.7244 - val_mape: 53.5599\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3539.8982 - mape: 46.7181 - val_loss: 3585.8130 - val_mape: 53.3542\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3520.6167 - mape: 46.4282 - val_loss: 3567.0955 - val_mape: 52.9357\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3500.7349 - mape: 46.0845 - val_loss: 3546.9851 - val_mape: 52.6718\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3481.7969 - mape: 45.8782 - val_loss: 3528.3074 - val_mape: 52.4221\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3469.4907 - mape: 45.6759 - val_loss: 3514.3730 - val_mape: 51.9913\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3449.7600 - mape: 45.5235 - val_loss: 3491.4143 - val_mape: 51.9960\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3434.3523 - mape: 45.5299 - val_loss: 3472.8982 - val_mape: 51.8204\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 3414.4524 - mape: 45.4257 - val_loss: 3452.3989 - val_mape: 51.8057\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3398.1970 - mape: 45.4366 - val_loss: 3436.5432 - val_mape: 51.6351\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3383.2195 - mape: 45.1886 - val_loss: 3422.0725 - val_mape: 51.3276\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3369.3572 - mape: 44.8991 - val_loss: 3408.5076 - val_mape: 51.0781\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3355.9050 - mape: 44.7105 - val_loss: 3393.1531 - val_mape: 50.7828\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3346.6506 - mape: 44.6450 - val_loss: 3375.0688 - val_mape: 50.7920\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3334.2498 - mape: 44.3657 - val_loss: 3368.6338 - val_mape: 50.2638\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3313.8716 - mape: 44.0891 - val_loss: 3354.1489 - val_mape: 50.0708\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3304.1965 - mape: 44.1866 - val_loss: 3335.2473 - val_mape: 50.1963\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3289.5200 - mape: 44.0361 - val_loss: 3325.9717 - val_mape: 49.8143\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3288.7549 - mape: 44.2171 - val_loss: 3306.6680 - val_mape: 49.9797\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3262.2451 - mape: 43.8622 - val_loss: 3300.8486 - val_mape: 49.3611\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3252.4060 - mape: 43.3769 - val_loss: 3290.8396 - val_mape: 49.0527\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3241.3525 - mape: 43.1040 - val_loss: 3283.2786 - val_mape: 48.7977\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3232.0376 - mape: 43.1492 - val_loss: 3266.9917 - val_mape: 48.8189\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3219.2986 - mape: 43.0575 - val_loss: 3258.6199 - val_mape: 48.5552\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3213.1536 - mape: 43.1544 - val_loss: 3240.0103 - val_mape: 48.7671\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3199.0872 - mape: 43.0012 - val_loss: 3234.5808 - val_mape: 48.3477\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3191.0625 - mape: 42.6603 - val_loss: 3227.2090 - val_mape: 48.1080\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3179.1267 - mape: 42.6842 - val_loss: 3212.1892 - val_mape: 48.1407\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3172.0198 - mape: 42.5725 - val_loss: 3205.2144 - val_mape: 47.9569\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3159.8086 - mape: 42.5295 - val_loss: 3193.6206 - val_mape: 47.8698\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3150.0215 - mape: 42.5632 - val_loss: 3179.7969 - val_mape: 47.8991\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3142.9146 - mape: 42.7539 - val_loss: 3167.8625 - val_mape: 47.9621\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3132.8564 - mape: 42.5043 - val_loss: 3164.2810 - val_mape: 47.5166\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3124.5071 - mape: 42.3431 - val_loss: 3154.9612 - val_mape: 47.4043\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3116.3110 - mape: 42.4039 - val_loss: 3142.8418 - val_mape: 47.5120\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3105.9744 - mape: 42.2091 - val_loss: 3137.1021 - val_mape: 47.2331\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3097.8611 - mape: 42.1688 - val_loss: 3126.8149 - val_mape: 47.1657\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3089.5166 - mape: 42.0952 - val_loss: 3117.4631 - val_mape: 46.9968\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3082.8257 - mape: 41.7643 - val_loss: 3115.5588 - val_mape: 46.5892\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3073.5925 - mape: 41.6617 - val_loss: 3107.3069 - val_mape: 46.4864\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3065.1567 - mape: 41.5432 - val_loss: 3098.9326 - val_mape: 46.3484\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3056.9949 - mape: 41.4523 - val_loss: 3090.7922 - val_mape: 46.2427\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3052.6367 - mape: 41.5623 - val_loss: 3079.0669 - val_mape: 46.3603\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3044.2375 - mape: 41.2989 - val_loss: 3081.9253 - val_mape: 45.8220\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3035.5518 - mape: 41.0409 - val_loss: 3074.2170 - val_mape: 45.7634\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3030.1450 - mape: 41.2687 - val_loss: 3057.7805 - val_mape: 46.1022\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3023.8093 - mape: 41.5075 - val_loss: 3047.9441 - val_mape: 46.1239\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3016.6309 - mape: 41.5504 - val_loss: 3040.7148 - val_mape: 46.0703\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 3004.4800 - mape: 41.2947 - val_loss: 3039.3420 - val_mape: 45.6289\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2998.3816 - mape: 40.8652 - val_loss: 3039.7764 - val_mape: 45.3432\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 2993.7568 - mape: 40.6276 - val_loss: 3037.5044 - val_mape: 45.1628\n",
      "CPU times: user 5.48 s, sys: 638 ms, total: 6.12 s\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# insert code here\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split = 0.25,\n",
    "    batch_size = 20, \n",
    "    epochs = 100,\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CR-kCu3c1Kd-"
   },
   "source": [
    "### Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkywKqPg1Kd-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 86.96007 ],\n",
       "       [144.26802 ],\n",
       "       [225.83986 ],\n",
       "       [183.18494 ],\n",
       "       [163.0701  ],\n",
       "       [210.43703 ],\n",
       "       [187.8062  ],\n",
       "       [204.24129 ],\n",
       "       [204.55666 ],\n",
       "       [254.64276 ],\n",
       "       [166.43462 ],\n",
       "       [135.39424 ],\n",
       "       [103.34682 ],\n",
       "       [170.81512 ],\n",
       "       [176.67412 ],\n",
       "       [138.87663 ],\n",
       "       [139.29698 ],\n",
       "       [106.40441 ],\n",
       "       [ 89.87723 ],\n",
       "       [198.31898 ],\n",
       "       [278.48434 ],\n",
       "       [140.05855 ],\n",
       "       [ 92.6588  ],\n",
       "       [135.11365 ],\n",
       "       [199.55074 ],\n",
       "       [131.26274 ],\n",
       "       [106.01468 ],\n",
       "       [188.19176 ],\n",
       "       [150.39067 ],\n",
       "       [153.43333 ],\n",
       "       [191.26715 ],\n",
       "       [155.23488 ],\n",
       "       [220.18404 ],\n",
       "       [184.53395 ],\n",
       "       [223.69717 ],\n",
       "       [179.8647  ],\n",
       "       [223.00755 ],\n",
       "       [139.16582 ],\n",
       "       [166.44542 ],\n",
       "       [130.08887 ],\n",
       "       [188.72993 ],\n",
       "       [226.55505 ],\n",
       "       [154.6792  ],\n",
       "       [141.21794 ],\n",
       "       [177.38333 ],\n",
       "       [122.05316 ],\n",
       "       [181.58452 ],\n",
       "       [181.20923 ],\n",
       "       [228.57445 ],\n",
       "       [ 92.4947  ],\n",
       "       [168.59465 ],\n",
       "       [256.14844 ],\n",
       "       [172.72157 ],\n",
       "       [160.44547 ],\n",
       "       [205.31873 ],\n",
       "       [170.10365 ],\n",
       "       [195.378   ],\n",
       "       [157.95601 ],\n",
       "       [138.68562 ],\n",
       "       [128.95898 ],\n",
       "       [189.9763  ],\n",
       "       [ 98.54059 ],\n",
       "       [119.73459 ],\n",
       "       [171.44463 ],\n",
       "       [176.22444 ],\n",
       "       [ 59.007504],\n",
       "       [138.08932 ],\n",
       "       [194.8684  ],\n",
       "       [238.71205 ],\n",
       "       [185.77235 ],\n",
       "       [116.728836],\n",
       "       [191.17331 ],\n",
       "       [135.1167  ],\n",
       "       [213.58012 ],\n",
       "       [280.2324  ],\n",
       "       [160.67017 ],\n",
       "       [250.42035 ],\n",
       "       [ 58.07657 ],\n",
       "       [118.80867 ],\n",
       "       [183.95062 ],\n",
       "       [ 89.63704 ],\n",
       "       [181.9157  ],\n",
       "       [170.8077  ],\n",
       "       [174.7613  ],\n",
       "       [182.60204 ],\n",
       "       [121.23056 ],\n",
       "       [147.86981 ],\n",
       "       [115.924576],\n",
       "       [100.36899 ],\n",
       "       [175.15611 ],\n",
       "       [176.62794 ],\n",
       "       [ 65.361115],\n",
       "       [172.07759 ],\n",
       "       [161.33487 ],\n",
       "       [116.6461  ],\n",
       "       [218.37761 ],\n",
       "       [215.42879 ],\n",
       "       [157.71724 ],\n",
       "       [218.46516 ],\n",
       "       [ 94.14177 ],\n",
       "       [171.56587 ],\n",
       "       [142.42809 ],\n",
       "       [128.05084 ],\n",
       "       [170.5193  ],\n",
       "       [152.34824 ],\n",
       "       [ 98.801926],\n",
       "       [198.4658  ],\n",
       "       [233.70256 ],\n",
       "       [ 75.52142 ],\n",
       "       [133.36406 ],\n",
       "       [142.65735 ],\n",
       "       [160.76097 ],\n",
       "       [100.24806 ],\n",
       "       [ 82.57655 ],\n",
       "       [129.88666 ],\n",
       "       [215.18968 ],\n",
       "       [117.77528 ],\n",
       "       [170.37282 ],\n",
       "       [176.15009 ],\n",
       "       [171.04253 ],\n",
       "       [218.9822  ],\n",
       "       [ 90.85684 ],\n",
       "       [171.72957 ],\n",
       "       [ 97.74276 ],\n",
       "       [203.17741 ],\n",
       "       [189.15228 ],\n",
       "       [152.60054 ],\n",
       "       [116.41099 ],\n",
       "       [193.90044 ],\n",
       "       [ 98.9101  ],\n",
       "       [ 95.41623 ],\n",
       "       [160.2863  ],\n",
       "       [241.82579 ]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert code here\n",
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-unEmrGo1KeA"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCIVpVrG1KeB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 839us/step - loss: 3334.6890 - mape: 39.8566\n"
     ]
    }
   ],
   "source": [
    "# insert code here\n",
    "score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3339.542724609375, 40.279239654541016]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want mape to be lower\n",
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.78877680492446"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find RMSE\n",
    "score[0] ** (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.13348416289594"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n",
      "346.0\n"
     ]
    }
   ],
   "source": [
    "print(y.min())\n",
    "print(y.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSlqjs7e1KeD"
   },
   "source": [
    "### Visualisation of cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3148.0966796875,\n",
       "  3143.23486328125,\n",
       "  3139.156494140625,\n",
       "  3132.754150390625,\n",
       "  3128.234375,\n",
       "  3125.193359375,\n",
       "  3118.872802734375,\n",
       "  3113.949462890625,\n",
       "  3109.658935546875,\n",
       "  3105.56494140625,\n",
       "  3100.68603515625,\n",
       "  3096.821533203125,\n",
       "  3092.072998046875,\n",
       "  3087.48291015625,\n",
       "  3082.8828125,\n",
       "  3079.23486328125,\n",
       "  3075.56298828125,\n",
       "  3071.63525390625,\n",
       "  3066.316650390625,\n",
       "  3061.546875,\n",
       "  3057.7509765625,\n",
       "  3053.67919921875,\n",
       "  3049.756103515625,\n",
       "  3045.01171875,\n",
       "  3043.8310546875,\n",
       "  3036.599609375,\n",
       "  3033.505859375,\n",
       "  3030.880126953125,\n",
       "  3024.953369140625,\n",
       "  3024.259765625,\n",
       "  3017.408935546875,\n",
       "  3016.774658203125,\n",
       "  3010.118896484375,\n",
       "  3006.613525390625,\n",
       "  3003.8896484375,\n",
       "  3000.151123046875,\n",
       "  2996.280517578125,\n",
       "  2993.00732421875,\n",
       "  2989.150390625,\n",
       "  2985.282470703125,\n",
       "  2982.22216796875,\n",
       "  2979.09912109375,\n",
       "  2975.524169921875,\n",
       "  2972.487548828125,\n",
       "  2968.951416015625,\n",
       "  2966.1669921875,\n",
       "  2962.373046875,\n",
       "  2959.48828125,\n",
       "  2956.710693359375,\n",
       "  2953.494384765625,\n",
       "  2951.17333984375,\n",
       "  2947.448486328125,\n",
       "  2946.51025390625,\n",
       "  2942.182373046875,\n",
       "  2938.02685546875,\n",
       "  2937.672607421875,\n",
       "  2933.88525390625,\n",
       "  2930.662109375,\n",
       "  2928.51611328125,\n",
       "  2924.1123046875,\n",
       "  2921.7646484375,\n",
       "  2918.819091796875,\n",
       "  2915.65478515625,\n",
       "  2913.049072265625,\n",
       "  2910.817138671875,\n",
       "  2908.30078125,\n",
       "  2907.24609375,\n",
       "  2902.8623046875,\n",
       "  2899.253173828125,\n",
       "  2896.725341796875,\n",
       "  2895.053466796875,\n",
       "  2893.021728515625,\n",
       "  2889.658447265625,\n",
       "  2886.74169921875,\n",
       "  2885.612548828125,\n",
       "  2880.9423828125,\n",
       "  2879.34033203125,\n",
       "  2880.211669921875,\n",
       "  2875.184326171875,\n",
       "  2872.76171875,\n",
       "  2870.624755859375,\n",
       "  2868.8603515625,\n",
       "  2866.291748046875,\n",
       "  2863.7568359375,\n",
       "  2861.2431640625,\n",
       "  2860.103515625,\n",
       "  2857.003173828125,\n",
       "  2856.453369140625,\n",
       "  2853.5390625,\n",
       "  2851.034912109375,\n",
       "  2851.31787109375,\n",
       "  2847.08447265625,\n",
       "  2845.2646484375,\n",
       "  2842.4150390625,\n",
       "  2840.550537109375,\n",
       "  2840.6005859375,\n",
       "  2836.422119140625,\n",
       "  2834.22021484375,\n",
       "  2835.51513671875,\n",
       "  2831.334228515625],\n",
       " 'mse': [3148.0966796875,\n",
       "  3143.2353515625,\n",
       "  3139.156494140625,\n",
       "  3132.754150390625,\n",
       "  3128.234375,\n",
       "  3125.193359375,\n",
       "  3118.872802734375,\n",
       "  3113.949462890625,\n",
       "  3109.658935546875,\n",
       "  3105.56494140625,\n",
       "  3100.68603515625,\n",
       "  3096.821533203125,\n",
       "  3092.072998046875,\n",
       "  3087.48291015625,\n",
       "  3082.8828125,\n",
       "  3079.23486328125,\n",
       "  3075.56298828125,\n",
       "  3071.63525390625,\n",
       "  3066.31689453125,\n",
       "  3061.546875,\n",
       "  3057.7509765625,\n",
       "  3053.67919921875,\n",
       "  3049.756103515625,\n",
       "  3045.011474609375,\n",
       "  3043.8310546875,\n",
       "  3036.599609375,\n",
       "  3033.505859375,\n",
       "  3030.880126953125,\n",
       "  3024.953125,\n",
       "  3024.259765625,\n",
       "  3017.408935546875,\n",
       "  3016.774658203125,\n",
       "  3010.118896484375,\n",
       "  3006.613525390625,\n",
       "  3003.8896484375,\n",
       "  3000.151123046875,\n",
       "  2996.280517578125,\n",
       "  2993.00732421875,\n",
       "  2989.150390625,\n",
       "  2985.282470703125,\n",
       "  2982.22216796875,\n",
       "  2979.09912109375,\n",
       "  2975.524169921875,\n",
       "  2972.487548828125,\n",
       "  2968.951416015625,\n",
       "  2966.1669921875,\n",
       "  2962.373046875,\n",
       "  2959.48828125,\n",
       "  2956.710693359375,\n",
       "  2953.494384765625,\n",
       "  2951.17333984375,\n",
       "  2947.448486328125,\n",
       "  2946.51025390625,\n",
       "  2942.182373046875,\n",
       "  2938.02685546875,\n",
       "  2937.672607421875,\n",
       "  2933.88525390625,\n",
       "  2930.662109375,\n",
       "  2928.51611328125,\n",
       "  2924.1123046875,\n",
       "  2921.7646484375,\n",
       "  2918.819091796875,\n",
       "  2915.65478515625,\n",
       "  2913.049072265625,\n",
       "  2910.817138671875,\n",
       "  2908.30078125,\n",
       "  2907.24609375,\n",
       "  2902.8623046875,\n",
       "  2899.253173828125,\n",
       "  2896.725341796875,\n",
       "  2895.053466796875,\n",
       "  2893.021728515625,\n",
       "  2889.658447265625,\n",
       "  2886.74169921875,\n",
       "  2885.612548828125,\n",
       "  2880.9423828125,\n",
       "  2879.34033203125,\n",
       "  2880.211669921875,\n",
       "  2875.184326171875,\n",
       "  2872.76171875,\n",
       "  2870.624755859375,\n",
       "  2868.8603515625,\n",
       "  2866.291748046875,\n",
       "  2863.7568359375,\n",
       "  2861.2431640625,\n",
       "  2860.103515625,\n",
       "  2857.003173828125,\n",
       "  2856.453369140625,\n",
       "  2853.5390625,\n",
       "  2851.034912109375,\n",
       "  2851.318115234375,\n",
       "  2847.08447265625,\n",
       "  2845.2646484375,\n",
       "  2842.4150390625,\n",
       "  2840.550537109375,\n",
       "  2840.6005859375,\n",
       "  2836.422119140625,\n",
       "  2834.22021484375,\n",
       "  2835.51513671875,\n",
       "  2831.334228515625],\n",
       " 'val_loss': [3054.018798828125,\n",
       "  3052.66552734375,\n",
       "  3051.837646484375,\n",
       "  3046.40625,\n",
       "  3043.32568359375,\n",
       "  3034.808837890625,\n",
       "  3031.777099609375,\n",
       "  3030.51953125,\n",
       "  3028.29443359375,\n",
       "  3025.877685546875,\n",
       "  3023.17578125,\n",
       "  3022.66748046875,\n",
       "  3018.753662109375,\n",
       "  3014.76025390625,\n",
       "  3012.540771484375,\n",
       "  3010.352783203125,\n",
       "  3005.232666015625,\n",
       "  3001.493896484375,\n",
       "  3001.076904296875,\n",
       "  2998.136474609375,\n",
       "  2995.36572265625,\n",
       "  2994.45654296875,\n",
       "  2990.52734375,\n",
       "  2989.39794921875,\n",
       "  2983.751708984375,\n",
       "  2984.277587890625,\n",
       "  2982.230224609375,\n",
       "  2977.501220703125,\n",
       "  2976.153564453125,\n",
       "  2970.945556640625,\n",
       "  2970.19189453125,\n",
       "  2971.107666015625,\n",
       "  2967.110107421875,\n",
       "  2965.143310546875,\n",
       "  2965.1240234375,\n",
       "  2960.325927734375,\n",
       "  2959.803955078125,\n",
       "  2958.465087890625,\n",
       "  2954.125,\n",
       "  2951.762939453125,\n",
       "  2950.093505859375,\n",
       "  2947.914794921875,\n",
       "  2946.5302734375,\n",
       "  2946.00634765625,\n",
       "  2943.3212890625,\n",
       "  2941.1162109375,\n",
       "  2939.868408203125,\n",
       "  2938.02392578125,\n",
       "  2935.177734375,\n",
       "  2933.344482421875,\n",
       "  2932.405517578125,\n",
       "  2930.9580078125,\n",
       "  2928.939697265625,\n",
       "  2927.740234375,\n",
       "  2923.313232421875,\n",
       "  2919.15234375,\n",
       "  2917.74609375,\n",
       "  2915.166748046875,\n",
       "  2916.121826171875,\n",
       "  2913.484375,\n",
       "  2912.878173828125,\n",
       "  2911.185546875,\n",
       "  2911.591064453125,\n",
       "  2910.617919921875,\n",
       "  2910.544189453125,\n",
       "  2907.503173828125,\n",
       "  2908.2392578125,\n",
       "  2905.1630859375,\n",
       "  2902.642333984375,\n",
       "  2901.121337890625,\n",
       "  2900.115966796875,\n",
       "  2897.794921875,\n",
       "  2898.25146484375,\n",
       "  2897.7880859375,\n",
       "  2894.936767578125,\n",
       "  2895.135498046875,\n",
       "  2894.714111328125,\n",
       "  2897.943359375,\n",
       "  2893.369873046875,\n",
       "  2890.929931640625,\n",
       "  2888.458984375,\n",
       "  2886.986083984375,\n",
       "  2885.027587890625,\n",
       "  2885.326416015625,\n",
       "  2884.588134765625,\n",
       "  2885.61767578125,\n",
       "  2885.753662109375,\n",
       "  2882.070068359375,\n",
       "  2878.944091796875,\n",
       "  2879.003173828125,\n",
       "  2876.9755859375,\n",
       "  2878.3505859375,\n",
       "  2876.09521484375,\n",
       "  2874.611328125,\n",
       "  2871.544189453125,\n",
       "  2868.76953125,\n",
       "  2868.96435546875,\n",
       "  2869.386962890625,\n",
       "  2873.04296875,\n",
       "  2871.802490234375],\n",
       " 'val_mse': [3054.018798828125,\n",
       "  3052.665771484375,\n",
       "  3051.837646484375,\n",
       "  3046.40625,\n",
       "  3043.32568359375,\n",
       "  3034.808837890625,\n",
       "  3031.777099609375,\n",
       "  3030.51953125,\n",
       "  3028.29443359375,\n",
       "  3025.877685546875,\n",
       "  3023.17578125,\n",
       "  3022.66748046875,\n",
       "  3018.753662109375,\n",
       "  3014.76025390625,\n",
       "  3012.540771484375,\n",
       "  3010.352783203125,\n",
       "  3005.232666015625,\n",
       "  3001.493896484375,\n",
       "  3001.076904296875,\n",
       "  2998.136474609375,\n",
       "  2995.36572265625,\n",
       "  2994.456298828125,\n",
       "  2990.52734375,\n",
       "  2989.39794921875,\n",
       "  2983.751708984375,\n",
       "  2984.277587890625,\n",
       "  2982.230224609375,\n",
       "  2977.501220703125,\n",
       "  2976.153564453125,\n",
       "  2970.945556640625,\n",
       "  2970.19189453125,\n",
       "  2971.107666015625,\n",
       "  2967.110107421875,\n",
       "  2965.143310546875,\n",
       "  2965.1240234375,\n",
       "  2960.325927734375,\n",
       "  2959.803955078125,\n",
       "  2958.465087890625,\n",
       "  2954.125,\n",
       "  2951.762939453125,\n",
       "  2950.093505859375,\n",
       "  2947.914794921875,\n",
       "  2946.5302734375,\n",
       "  2946.00634765625,\n",
       "  2943.3212890625,\n",
       "  2941.1162109375,\n",
       "  2939.868408203125,\n",
       "  2938.02392578125,\n",
       "  2935.177734375,\n",
       "  2933.344482421875,\n",
       "  2932.405517578125,\n",
       "  2930.957763671875,\n",
       "  2928.939697265625,\n",
       "  2927.740234375,\n",
       "  2923.313232421875,\n",
       "  2919.15234375,\n",
       "  2917.74609375,\n",
       "  2915.166748046875,\n",
       "  2916.121826171875,\n",
       "  2913.484375,\n",
       "  2912.878173828125,\n",
       "  2911.185546875,\n",
       "  2911.591064453125,\n",
       "  2910.617919921875,\n",
       "  2910.544189453125,\n",
       "  2907.503173828125,\n",
       "  2908.239501953125,\n",
       "  2905.1630859375,\n",
       "  2902.642333984375,\n",
       "  2901.12109375,\n",
       "  2900.115966796875,\n",
       "  2897.794921875,\n",
       "  2898.25146484375,\n",
       "  2897.7880859375,\n",
       "  2894.936767578125,\n",
       "  2895.135498046875,\n",
       "  2894.714111328125,\n",
       "  2897.943359375,\n",
       "  2893.369873046875,\n",
       "  2890.929931640625,\n",
       "  2888.458984375,\n",
       "  2886.986083984375,\n",
       "  2885.027587890625,\n",
       "  2885.326416015625,\n",
       "  2884.588134765625,\n",
       "  2885.61767578125,\n",
       "  2885.753662109375,\n",
       "  2882.070068359375,\n",
       "  2878.944091796875,\n",
       "  2879.003173828125,\n",
       "  2876.9755859375,\n",
       "  2878.3505859375,\n",
       "  2876.09521484375,\n",
       "  2874.61181640625,\n",
       "  2871.544189453125,\n",
       "  2868.76953125,\n",
       "  2868.96435546875,\n",
       "  2869.386962890625,\n",
       "  2873.04296875,\n",
       "  2871.802490234375]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mse'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a198ed373676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plot training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model MSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mse'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize = (18, 6))\n",
    "fig.subplots_adjust(left = 0.02, right = 0.98, wspace = 0.2)\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "ax[0].plot(history.history['mse'], label = 'Training')\n",
    "ax[0].plot(history.history['mape'], label = 'Validation')\n",
    "ax[0].set_title('Model MSE')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "ax[1].plot(history.history['mse'], label = 'Training')\n",
    "ax[1].plot(history.history['mape'], label = 'Validation')\n",
    "ax[1].set_title('Model loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1F6hYpNh1KeE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# insert code here\n",
    "def print_cm(cm):\n",
    "    c = '%%%dd ' % len('%d' % cm.max())\n",
    "    s = ' | '\n",
    "    s += ''.join([c % i for i in range(len(cm[0]))])\n",
    "    print(s)\n",
    "    print('-' * len(s))\n",
    "    for i, r in enumerate(cm):\n",
    "        s = '%d| ' % i\n",
    "        s += c * len(r)\n",
    "        print(s % tuple(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2757b274890f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_test_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 91\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "y_test_target = np.array([x.argmax() for x in y_test])\n",
    "cm = confusion_matrix(y_test_target, predictions)\n",
    "print_cm(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQLzMZTM1KeG"
   },
   "source": [
    "### Visualisation of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opIulVSw1KeH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45833840949904703"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insert code here\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IS5Tc4z9FoYy"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxI2We9OFpfs"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81DoNxN1FqGN"
   },
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RERADKgNFq9T"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > >  2019 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Lab-10_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
